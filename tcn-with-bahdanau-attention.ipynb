{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport json\nimport math\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_addons as tfa","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Copied from previous comp 1st place model: https://www.kaggle.com/code/hoyso48/1st-place-solution-training\nclass ECA(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=5, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.kernel_size = kernel_size\n        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n\n    def call(self, inputs, mask=None):\n        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n        nn = tf.expand_dims(nn, -1)\n        nn = self.conv(nn)\n        nn = tf.squeeze(nn, -1)\n        nn = tf.nn.sigmoid(nn)\n        nn = nn[:,None,:]\n        return inputs * nn\n\nclass CausalDWConv1D(tf.keras.layers.Layer):\n    def __init__(self, \n        kernel_size=17,\n        dilation_rate=1,\n        use_bias=False,\n        depthwise_initializer='glorot_uniform',\n        name='', **kwargs):\n        super().__init__(name=name,**kwargs)\n        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n                            kernel_size,\n                            strides=1,\n                            dilation_rate=dilation_rate,\n                            padding='valid',\n                            use_bias=use_bias,\n                            depthwise_initializer=depthwise_initializer,\n                            name=name + '_dwconv')\n        self.supports_masking = True\n        \n    def call(self, inputs):\n        x = self.causal_pad(inputs)\n        x = self.dw_conv(x)\n        return x\n\ndef Conv1DBlock(channel_size,\n          kernel_size,\n          dilation_rate=1,\n          drop_rate=0.0,\n          expand_ratio=2,\n          se_ratio=0.25,\n          activation='swish',\n          name=None):\n    '''\n    efficient conv1d block, @hoyso48\n    '''\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n    # Expansion phase\n    def apply(inputs):\n        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n        channels_expand = channels_in * expand_ratio\n\n        skip = inputs\n\n        x = tf.keras.layers.Dense(\n            channels_expand,\n            use_bias=True,\n            activation=activation,\n            name=name + '_expand_conv')(inputs)\n\n        # Depthwise Convolution\n        x = CausalDWConv1D(kernel_size,\n            dilation_rate=dilation_rate,\n            use_bias=False,\n            name=name + '_dwconv')(x)\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n\n        x  = ECA()(x)\n\n        x = tf.keras.layers.Dense(\n            channel_size,\n            use_bias=True,\n            name=name + '_project_conv')(x)\n\n        if drop_rate > 0:\n            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n\n        if (channels_in == channel_size):\n            x = tf.keras.layers.add([x, skip], name=name + '_add')\n        return x\n\n    return apply","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        query_with_time_axis = tf.expand_dims(query, 1)\n        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def TCNBlock(filters, kernel_size, dilation_rate, drop_rate=0.0, activation='relu', name=None):\n    \"\"\"\n    Temporal Convolutional Block with Bahdanau Attention.\n    \"\"\"\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"tcnblock\"))\n    \n    attention_layer = BahdanauAttention(filters)\n    \n    def apply(inputs):\n        skip = inputs\n\n        # 1. Causal Convolution\n        x = CausalDWConv1D(kernel_size, dilation_rate=dilation_rate, name=name + '_dwconv')(inputs)\n        \n        # 2. Batch Normalization and Activation\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn1')(x)\n        x = tf.keras.layers.Activation(activation, name=name + '_act1')(x)\n\n        # 3. Efficient Channel Attention\n        x = ECA()(x)\n\n        # 4. Apply Bahdanau Attention\n        query = tf.keras.layers.Dense(filters)(x)\n        x, attention_weights = attention_layer(query, x)\n\n        # 5. Pointwise Convolution\n        x = tf.keras.layers.Conv1D(filters, 1, activation=activation, name=name + '_conv1')(x)\n        \n        # 6. Another round of Batch Normalization\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn2')(x)\n        \n        # 7. Dropout for regularization\n        x = tf.keras.layers.Dropout(drop_rate, name=name + '_drop')(x)\n        \n        # 8. Project input to the right shape for residual connection\n        if tf.keras.backend.int_shape(skip)[-1] != filters:\n            skip = tf.keras.layers.Conv1D(filters, 1, name=name + '_residual_proj')(skip)\n            \n        # 9. Residual connection\n        x = tf.keras.layers.add([x, skip], name=name + '_add')\n        \n        return x\n\n    return apply\n","metadata":{},"execution_count":null,"outputs":[]}]}